{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorstore-backed Memory\n",
    "\n",
    "This means storing the personality preferences in a data structure that's easy to look up. In the case of LLMs, since Q&A is unstructured data and queries, a vector database might come to mind. This is not the only option (can always convert unstructured queries to structured, and query data in SQL or other ways).\n",
    "\n",
    "Pinecone setup to store user's personality bank , and only query the relevant parts when the assistant looks at the user's query.\n",
    "\n",
    "First, we set up Pinecone as our vector database.\n",
    "\n",
    "Step one is to install Pinecone.\n",
    "https://github.com/trancethehuman/ai-workshop-code/blob/main/Long_term_memory_%26_personalized_LLM_responses.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pinecone-client --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY = getpass.getpass('Enter your Pinecone API key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup Pinecone SDK client. using Pinecone's new serverless architecture (free lol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create an index. An index for a vector database is a way to organize embeddings in an efficient manner for search. Popular indexing algorithms are: HNSW, IVF, etc.. I use HNSW with Supabase (which is just a Postgres table and pg-vector extension underneath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"user-preferences\"\n",
    "\n",
    "EMBEDDINGS_DIMENSIONS = 512\n",
    "\n",
    "# Just checking to see if this index already exist\n",
    "existing_indexes = pc.list_indexes().names()\n",
    "\n",
    "# If index doesn't exist yet, then delete it and create one (we're starting from scratch)\n",
    "if index_name in existing_indexes:\n",
    "  pc.delete_index(index_name)\n",
    "  print(\"Deleted index.\")\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=EMBEDDINGS_DIMENSIONS,\n",
    "    metric=\"cosine\",\n",
    "    spec=ServerlessSpec(\n",
    "        cloud='aws',\n",
    "        region='us-east-1'\n",
    "    )\n",
    ")\n",
    "print(\"Created index.\")\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "few things about this setup:\n",
    "\n",
    "- \"cosine\" stands for cosine similarity, which is a way to calculate distance between vectors in 3D space. It's just one way to do vector search.\n",
    "- dimension: How many dimensions does embedding model create for each piece of text? Heavily fine-tuned models with high quality data can get away with lower numbers (like OpenAI's recent text-embedding-3-large can throw away half the dimensions and still perform as well as dumber models that require more dimensions to represent the same concept. \n",
    "went with 512 because that's an optimal number for this chosen embeddings model. Lower dimensions also saves  database storage and potentially save lots of money.\n",
    "also need to setup embedding model so easily convert text to numbers for easy vector search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain-openai --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedding_client = OpenAIEmbeddings(api_key=OPENAI_API_KEY,\n",
    "    model=\"text-embedding-3-small\", dimensions=EMBEDDINGS_DIMENSIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now update chat pipeline to do a search of the user's existing preferences in the vector index to get similar things. This will help LLM make a better decision when recommending things.\n",
    "\n",
    "need to:\n",
    "- Embed the latest user query. This allows us to use the embeddings to find similar data in user's preferences.\n",
    "- Find the list of existing preferences.\n",
    "- Let the LLM know that these are the preferences that are relevant to the latest query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here's a handy function to just print the streaming content to our terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = {\"role\": \"system\", \"content\": \"You are a helpful personal assistant. Your main goal is to take into account what you know about the user and answer them in pirate speak.\"}\n",
    "messages_history = [system_message]\n",
    "\n",
    "\n",
    "def print_ai_answer(user_input: str):\n",
    "    for chunk in get_ai_answer(user_input, messages_history):\n",
    "        print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_personality(user_input: str):\n",
    "  entity_extraction_system_message = {\"role\": \"system\", \"content\": \"Looking at the user's message, you must extract their likes and dislikes and put them as strings into lists, and respond only in JSON in this format: {{\"\"likes\"\": \"\"[]\"\", \"\"dislikes\"\": \"\"[]\"\"}}\"}\n",
    "\n",
    "  messages = [entity_extraction_system_message]\n",
    "  messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "  response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        stream=False,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "\n",
    "  return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# Declare this here just to wipe the slate clean again.\n",
    "extracted_personality = {\"likes\": [], \"dislikes\": []}\n",
    "\n",
    "def print_ai_answer(user_input: str):\n",
    "  # Clear the current messages history for fair testing :)\n",
    "  messages_history = []\n",
    "  print(\"Messages history cleared.\")\n",
    "\n",
    "  # Extract relevant personality info and update personality dictionary\n",
    "  new_personalities = json.loads(extract_personality(user_input))\n",
    "  print(\"Done extracting personality.\")\n",
    "  extracted_personality[\"likes\"].extend(new_personalities[\"likes\"])\n",
    "  extracted_personality[\"dislikes\"].extend(new_personalities[\"dislikes\"])\n",
    "\n",
    "  # Embed each new personality item, give them metadata so we can filter later, and upsert them to our vector database\n",
    "  embeddings = []\n",
    "  for dislike in extracted_personality[\"dislikes\"]:\n",
    "    text_to_embed = f\"The user dislikes {dislike}\"\n",
    "    current_embeddings = embedding_client.embed_query(text_to_embed)\n",
    "\n",
    "    dislike_with_metadata = {\n",
    "        \"id\": str(uuid.uuid4()), \"values\": current_embeddings, \"metadata\": {\"type\": \"dislikes\", \"content\": dislike}\n",
    "    }\n",
    "    embeddings.append(dislike_with_metadata)\n",
    "\n",
    "  for like in extracted_personality[\"likes\"]:\n",
    "    text_to_embed = f\"The user likes {like}\"\n",
    "    current_embeddings = embedding_client.embed_query(text_to_embed)\n",
    "\n",
    "    dislike_with_metadata = {\n",
    "        \"id\": str(uuid.uuid4()), \"values\": current_embeddings, \"metadata\": {\"type\": \"likes\", \"content\": like}\n",
    "    }\n",
    "    embeddings.append(dislike_with_metadata)\n",
    "\n",
    "  # Push all of our embeddings (likes and dislikes) to vector database (Pinecone)\n",
    "  index.upsert(vectors=embeddings)\n",
    "\n",
    "  # Embed the user's question so we can compare to our embedded personality items\n",
    "  user_query_embedded = embedding_client.embed_query(user_input)\n",
    "\n",
    "  # Search for relevant personalities. Here, to make my life easier, we just look up things the person likes\n",
    "  likes_filter={\n",
    "        \"type\": {\"$eq\": \"likes\"}\n",
    "    }\n",
    "\n",
    "  found_likes = index.query(\n",
    "      vector=user_query_embedded,\n",
    "      filter=likes_filter,\n",
    "      top_k=1, # we only want one piece of personality trait from our bank\n",
    "      include_values=True,\n",
    "      include_metadata=True\n",
    "  )\n",
    "\n",
    "\n",
    "  def get_content_out_of_pinecone_query_result():\n",
    "    content_list = []\n",
    "    # Loop through each match in the query result\n",
    "    for match in found_likes.get('matches', []):\n",
    "        # Get the 'metadata' dictionary from the match\n",
    "        metadata = match.get('metadata', {})\n",
    "        # Extract the 'content' from the 'metadata'\n",
    "        if 'content' in metadata:\n",
    "            content_list.append(metadata['content'])\n",
    "\n",
    "    return content_list\n",
    "\n",
    "  found_likes_formatted = get_content_out_of_pinecone_query_result()\n",
    "\n",
    "  print(\"Found the following relevant things that the user liked in the past:\")\n",
    "  pprint(found_likes_formatted)\n",
    "\n",
    "  # Insert our user's likes into the messages_history list\n",
    "  messages_history.append({\"role\": \"assistant\", \"content\": f\"The user really likes: {found_likes_formatted}. This is what you know about the user. Use it in your answer.\"})\n",
    "\n",
    "  # Generate a final answer\n",
    "  for chunk in get_ai_answer(user_input, messages_history):\n",
    "      print(chunk, end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
