{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5afcaed0-3d55-4e1f-95d3-c32c751c29d8",
   "metadata": {
    "id": "5afcaed0-3d55-4e1f-95d3-c32c751c29d8"
   },
   "source": [
    "# Adaptive RAG \n",
    "\n",
    "Adaptive RAG is a strategy for RAG that unites (1) [query analysis](https://blog.langchain.dev/query-construction/) with (2) [active / self-corrective RAG](https://blog.langchain.dev/agentic-rag-with-langgraph/).\n",
    "\n",
    "interesting cases:\n",
    "\n",
    "* No Retrieval (LLM answers)\n",
    "* Single-shot RAG\n",
    "* Iterative RAG\n",
    "  \n",
    "![Screenshot 2024-04-02 at 8.11.18 PM.png](data\\AdaptiveRAG.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6c329ba-cb85-4576-9828-4f2ac648d1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install --quiet langchain langchain_cohere langchain-openai tiktoken langchainhub chromadb langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "222f204d-956f-4128-b597-2c698120edda",
   "metadata": {
    "id": "222f204d-956f-4128-b597-2c698120edda"
   },
   "outputs": [],
   "source": [
    "### LLMs\n",
    "import os\n",
    "os.environ['COHERE_API_KEY'] = os.getenv(\"COHERE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08edba00-988a-478b-96fc-ae0199cbef49",
   "metadata": {
    "id": "08edba00-988a-478b-96fc-ae0199cbef49"
   },
   "outputs": [],
   "source": [
    "# ### Tracing \n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1c2cd-81fb-40eb-8ba1-e9197800cba6",
   "metadata": {
    "id": "9ac1c2cd-81fb-40eb-8ba1-e9197800cba6"
   },
   "source": [
    "## Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b224e5ba-50ca-495a-a7fa-0f75a080e03c",
   "metadata": {
    "id": "b224e5ba-50ca-495a-a7fa-0f75a080e03c"
   },
   "outputs": [],
   "source": [
    "### Build Index\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "# Set embeddings\n",
    "embd = CohereEmbeddings()\n",
    "\n",
    "# Docs to index\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\",\n",
    "    \"https://txt.cohere.com/commonly-asked-questions-about-search-from-coheres-enterprise-customers/\",\n",
    "    \"https://txt.cohere.com/chat-finetuning-guide/\",\n",
    "    \"https://txt.cohere.com/tool-use-with-command-r/\",\n",
    "]\n",
    "\n",
    "# Load\n",
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=512, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorstore\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    embedding=embd,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36f3fd51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Fine-Tuning with CohereCohere offers two methods of creating fine-tuned models: via the fine-tuning dashboard and the Python SDK.\\xa0\\xa0The fine-tuning dashboard is a no-code option that allows you to get started quickly.The fine-tuning dashboard makes it easy to manage and run fine-tuning projectsThe Python SDK allows you to kick off fine-tuning jobs programmatically. This is useful if, for example, you plan to run multiple fine-tuning jobs on a regular schedule.\\xa0\\xa0Step-by-Step Guide with the Python SDKIn this article, we‚Äôll explore the steps involved in training a custom chatbot using the Python SDK for the code example.\\xa0Follow along in the notebook.\\xa0Step 1: Prepare and Validate the DatasetWe will work with the CoEdIT dataset1 of text editing examples (Raheja, et al). In each example, the user asks a writing assistant to rewrite text to suit a specific task (editing fluency, coherence, clarity, or style) and receives a response. Below, you can see some examples from the raw dataset.{\"_id\": \"57241\", \"task\": \"coherence\", \"src\": \"Make the text more coherent: It lasted for 60 minutes. It featured the three men taking questions from a studio audience.\", \"tgt\": \"Lasting for 60 minutes, it featured the three men taking questions from a studio audience.\"}\\n\\n{\"_id\": \"69028\", \"task\": \"clarity\", \"src\": \"Make the sentence clearer: URLe Lilanga (1934 27 June 2005) was a Tanzanian painter and sculptor, active from the late 1970s and until the early years of the 21st century.\", \"tgt\": \"URLe Lilanga (1934 27 June 2005) was a Tanzanian painter and sculptor, active from the late 1970s and until the early 21st century.\"}', metadata={'description': 'How to fine-tune a chatbot to improve its performance at a specific task.', 'language': 'en', 'source': 'https://txt.cohere.com/chat-finetuning-guide/', 'title': \"The Developer's Guide to Fine-Tuning Cohere Chat\"}),\n",
       " Document(page_content=\"User: Help me with this one - She left Benaras. Conditions back home were bad.\\nChatbot: She left Benaras because conditions back home were bad.\\n--------------------------------------------------\\n\\nUser: What's a good time to visit London\\nChatbot: A good time to visit London is in the spring or fall, when the weather is mild and the city is not too crowded.  The best time to visit London depends on what you want to see and do.  If you want to see the sights and do some shopping, a good time to visit London is in the spring, when the weather is mild and the city is not too crowded.  If you want to see the sights and do some sightseeing, a good time to visit London is in the fall, when the weather is mild and the city is not too crowded.\\n--------------------------------------------------\\n\\nUser: Could you help with this please: Make the text coherent: Critically the album has not been as well received as other Browne recordings. It remains his only album to date to reach number 1 on the Billboard chart. \\nChatbot: Critically the album has not been as well received as other Browne recordings, but it remains his only album to date to reach number 1 on the Billboard chart.\\n--------------------------------------------------\\n\\nUser: quit\\nEnding chat.\", metadata={'description': 'How to fine-tune a chatbot to improve its performance at a specific task.', 'language': 'en', 'source': 'https://txt.cohere.com/chat-finetuning-guide/', 'title': \"The Developer's Guide to Fine-Tuning Cohere Chat\"}),\n",
       " Document(page_content=\"Experiment results on the GHC (Gab Hate Corpus) dataset showed that the multi-task model achieves the best F1 score and also can naturally provide prediction uncertainty estimation, correlated with annotation disagreement.\\n\\nFig. 6. Illustration of different architectures for modeling multiple annotators' labels. (Image source: Davani et al. 2021)\\nJury Learning (Gordon et al. 2022) mimics the jury process by modeling the different annotators‚Äô labeling behavior conditioned on their characteristics. Starting with a dataset with labels and demographic characteristics of each labeler, we train a model to learn to predict labels made by every individual annotator, each as a potential juror. At decision time, practitioners can specify the composition of a group of jurors to determine a sampling strategy. The final decision is made by aggregating labels from jurors from multiple trials.\\n\\nFig. 7. Illustration of how jury learning works. (Image source: Gordon et al. 2022)\\nThe jury learning model is a DCN (Deep & Cross network) , commonly for recommendation use case,  that is jointly trained to learn comment embedding, annotator embedding and group (annotator‚Äôs characteristics) embedding. The text content is processed by a pre-trained BERT, which is also jointly fine-tuned but for a shorter period to avoid overfitting.\\n\\nFig. 8. DCN model architecture for jury learning. (Image source: Gordon et al. 2022)\\nTheir experiment runs on the toxicity diversity dataset and compares jury learning with a baseline model which is a fine-tuned BERT to predict individual annotator‚Äôs label without using metadata. Performance is measured in MAE (mean absolute error). Jury learning consistently outperforms the annotator-agnostic baseline on the full test set as well as each group segment.\", metadata={'description': '[Special thank you to Ian Kivlichan for many useful pointers (E.g. the 100+ year old Nature paper ‚ÄúVox populi‚Äù) and nice feedback. üôè ]\\nHigh-quality data is the fuel for modern data deep learning model training. Most of the task-specific labeled data comes from human annotation, such as classification task or RLHF labeling (which can be constructed as classification format) for LLM alignment training. Lots of ML techniques in the post can help with data quality, but fundamentally human data collection involves attention to details and careful execution.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2024-02-05-human-data-quality/', 'title': \"Thinking about High-Quality Human Data | Lil'Log\"}),\n",
       " Document(page_content='call\\xa0Execute the tool call. The developer will run the tool and deliver the output back to Command RAs a result, the LLM will generate an answer based on tool results. Command R uses the tools‚Äô output to generate a response to the end userTool Use enables a richer set of behaviors for Command R. For example, it allows the language model to interact with data hosted in external sources, and to take action on enterprise tools beyond just retrieval.Example: Keep Your CRM Up-To-Date AutomaticallyTo boost enterprise productivity, users can automate updates to their CRM based on the latest interactions with their target customers.\\xa0Here‚Äôs how it works.\\xa0A sales manager might want the CRM to automatically update with new information after a call with a target customer, instead of spending time manually making the changes. For example, using two tools, you can update your CRM with new points of contact based on a customer call transcript.The first tool retrieves all the information about a target customer from the CRM system. Given this information alongside a transcript of the sales call, Command-R identifies which fields in the CRM need to be changed with new data and generates the instruction.\\xa0Now with the CRM instructions, a separate second tool can be directed to write to the CRM, updating the fields that it had previously suggested. The CRM is now updated with the names and roles identified previously and has the most up-to-date information based on the latest customer call.Harnessing automations to update a CRM enhances efficiency and ensures that your customer data is always current. This workflow, powered by Command-R and Tool Use, streamlines the manual task of entering data, letting the sales teams focus on building relationships and closing deals.Get StartedKeeping your CRM up-to-date automatically is just one of many applications that can be set up with Tool Use.Developers can get started with and learn more about our Tool Use API in our developer documentation. Check out our notebook for a tutorial on how to set up a tool, we‚Äôre excited to see what you build! Share your tools in our Discord Community.Coming Soon‚Ä¶Multi-hop Tool Use. If you want to join the beta, please reach out!', metadata={'description': 'Tool Use enables language models to interact with user-defined tools to automate highly sophisticated tasks.', 'language': 'en', 'source': 'https://txt.cohere.com/tool-use-with-command-r/', 'title': 'Introducing Tool Use With Command R: Seamlessly Automate Business Workflows'})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"connector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f52b427-750c-40f8-8893-e9caab3afd8d",
   "metadata": {
    "id": "0f52b427-750c-40f8-8893-e9caab3afd8d"
   },
   "source": [
    "## LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "H5CztTqsBOTZ",
   "metadata": {
    "id": "H5CztTqsBOTZ"
   },
   "source": [
    "We use a router to pick between tools. \n",
    " \n",
    "Cohere model decides which tool(s) to call, as well as the how to query them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bYK-e0diGdPf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bYK-e0diGdPf",
    "outputId": "895a8ea5-57ee-49fe-ef28-277eac8a7bb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '960eff47-a953-4fe9-8936-0d94ab37b572', 'function': {'name': 'web_search', 'arguments': '{\"query\": \"best multivitamins for men\"}'}, 'type': 'function'}]\n",
      "[{'id': 'b3b6c292-0ee5-49f0-be7d-a07caba26394', 'function': {'name': 'vectorstore', 'arguments': '{\"query\": \"build rag applications web search\"}'}, 'type': 'function'}]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "### Router\n",
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "# Data model\n",
    "class web_search(BaseModel):\n",
    "    \"\"\"\n",
    "    The internet. Use web_search for questions that are related to anything else than agents, cohere, connectors and human data.\n",
    "    \"\"\"\n",
    "    query: str = Field(description=\"The query to use when searching the internet.\")\n",
    "\n",
    "class vectorstore(BaseModel):\n",
    "    \"\"\"\n",
    "    A vectorstore containing documents related to agents, cohere, connectors and human data. Use the vectorstore for questions on these topics.\n",
    "    \"\"\"\n",
    "    query: str = Field(description=\"The query to use when searching the vectorstore.\")\n",
    "\n",
    "# Preamble\n",
    "preamble = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
    "The vectorstore contains documents related to agents, cohere, connectors and human data.\n",
    "Use the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\n",
    "\n",
    "# LLM with tool use and preamble\n",
    "llm = ChatCohere(model=\"command-r\", temperature=0)\n",
    "structured_llm_router = llm.bind_tools(tools=[web_search, vectorstore], preamble=preamble)\n",
    "\n",
    "# Prompt\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_router = route_prompt | structured_llm_router\n",
    "response = question_router.invoke({\"question\": \"What are the best multivitamins for men?\"})\n",
    "print(response.response_metadata['tool_calls'])\n",
    "response = question_router.invoke({\"question\": \"How to build RAG applications With web search?\"})\n",
    "print(response.response_metadata['tool_calls'])\n",
    "response = question_router.invoke({\"question\": \"Hi how are you?\"})\n",
    "print('tool_calls' in response.response_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "oaLWNbWxBgjE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oaLWNbWxBgjE",
    "outputId": "57a5c27b-044b-4df5-f55d-7bf23d3976d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bahar\\Desktop\\Sia\\Orixa\\env\\lib\\site-packages\\langchain_core\\_api\\beta_decorator.py:87: LangChainBetaWarning: The function `with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='no'\n"
     ]
    }
   ],
   "source": [
    "### Retrieval Grader\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "# Prompt\n",
    "preamble = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatCohere(model=\"command-r\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments, preamble=preamble)\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "question = \"types of agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "response =  retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D43a7vM4EElX",
   "metadata": {
    "id": "D43a7vM4EElX"
   },
   "source": [
    "Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "BTIUdjRMEq_h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BTIUdjRMEq_h",
    "outputId": "11a99f62-2449-45db-9281-5bfd60e3c966"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, I don't know.\n"
     ]
    }
   ],
   "source": [
    "### Generate\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import langchain\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "# Preamble\n",
    "preamble = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\"\"\"\n",
    "\n",
    "# LLM\n",
    "llm = ChatCohere(model_name=\"command-r\", temperature=0).bind(preamble=preamble)\n",
    "\n",
    "# Prompt\n",
    "prompt = lambda x: ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            f\"Question: {x['question']} \\nAnswer: \",\n",
    "            additional_kwargs={\"documents\": x[\"documents\"]},\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"documents\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc000a7d-84b6-4eb2-88ad-65cc62a44431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have feelings as an AI chatbot, but I'm here to assist you with any queries or concerns you may have. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "### LLM fallback\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import langchain\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "# Preamble\n",
    "preamble = \"\"\"You are an assistant for question-answering tasks. Answer the question based upon your knowledge. Use three sentences maximum and keep the answer concise.\"\"\"\n",
    "\n",
    "# LLM\n",
    "llm = ChatCohere(model_name=\"command-r\", temperature=0).bind(preamble=preamble)\n",
    "\n",
    "# Prompt\n",
    "prompt = lambda x: ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        HumanMessage(\n",
    "            f\"Question: {x['question']} \\nAnswer: \"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Chain\n",
    "llm_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "question = \"Hi how are you?\"\n",
    "generation = llm_chain.invoke({\"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "y0msuR2DHQkY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y0msuR2DHQkY",
    "outputId": "f0e91c2a-5542-45c0-a7a6-60453e0b2bc4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeHallucinations(binary_score='yes')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Hallucination Grader\n",
    "\n",
    "# Data model\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Answer is grounded in the facts, 'yes' or 'no'\")\n",
    "\n",
    "# Preamble\n",
    "preamble = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
    "Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatCohere(model=\"command-r\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucinations, preamble=preamble)\n",
    "\n",
    "# Prompt\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        # (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0c08d14-77a0-4eed-b882-2d636abb22a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0c08d14-77a0-4eed-b882-2d636abb22a3",
    "outputId": "c4f88c9a-65fd-4dad-e739-3c9bd547a9f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeAnswer(binary_score='yes')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Answer Grader\n",
    "\n",
    "# Data model\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(description=\"Answer addresses the question, 'yes' or 'no'\")\n",
    "\n",
    "# Preamble\n",
    "preamble = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n\n",
    "Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatCohere(model=\"command-r\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer, preamble=preamble)\n",
    "\n",
    "# Prompt\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | structured_llm_grader\n",
    "answer_grader.invoke({\"question\": question,\"generation\": generation})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07c0b31-b919-4498-869f-9673125c2473",
   "metadata": {
    "id": "d07c0b31-b919-4498-869f-9673125c2473"
   },
   "source": [
    "## Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01d829bb-1074-4976-b650-ead41dcb9788",
   "metadata": {
    "id": "01d829bb-1074-4976-b650-ead41dcb9788"
   },
   "outputs": [],
   "source": [
    "### Search\n",
    "# os.environ['TAVILY_API_KEY'] = <your-api-key>\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "web_search_tool = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbbff0e-8843-45bb-b2ff-137bef707ef4",
   "metadata": {
    "id": "efbbff0e-8843-45bb-b2ff-137bef707ef4"
   },
   "source": [
    "# Graph\n",
    "\n",
    "Capture the flow in as a graph.\n",
    "\n",
    "## Graph state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e723fcdb-06e6-402d-912e-899795b78408",
   "metadata": {
    "id": "e723fcdb-06e6-402d-912e-899795b78408"
   },
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"|\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "    question : str\n",
    "    generation : str\n",
    "    documents : List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2d6c0d-42e8-4399-9751-e315be16607a",
   "metadata": {
    "id": "7e2d6c0d-42e8-4399-9751-e315be16607a"
   },
   "source": [
    "## Graph Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b76b5ec3-0720-443d-85b1-c0e79659ca0a",
   "metadata": {
    "id": "b76b5ec3-0720-443d-85b1-c0e79659ca0a"
   },
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def llm_fallback(state):\n",
    "    \"\"\"\n",
    "    Generate answer using the LLM w/o vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---LLM Fallback---\")\n",
    "    question = state[\"question\"]\n",
    "    generation = llm_chain.invoke({\"question\": question})\n",
    "    return {\"question\": question, \"generation\": generation}\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer using the vectorstore\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    if not isinstance(documents, list):\n",
    "      documents = [documents]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"documents\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke({\"question\": question, \"document\": d.page_content})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}\n",
    "\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Web search\n",
    "    docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
    "    web_results = Document(page_content=web_results)\n",
    "\n",
    "    return {\"documents\": web_results, \"question\": question}\n",
    "\n",
    "### Edges ###\n",
    "\n",
    "def route_question(state):\n",
    "    \"\"\"\n",
    "    Route question to web search or RAG.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ROUTE QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    source = question_router.invoke({\"question\": question})\n",
    "    \n",
    "    # Fallback to LLM or raise error if no decision\n",
    "    if \"tool_calls\" not in source.additional_kwargs:\n",
    "        print(\"---ROUTE QUESTION TO LLM---\")\n",
    "        return \"llm_fallback\" \n",
    "    if len(source.additional_kwargs[\"tool_calls\"]) == 0:\n",
    "      raise \"Router could not decide source\"\n",
    "\n",
    "    # Choose datasource\n",
    "    datasource = source.additional_kwargs[\"tool_calls\"][0][\"function\"][\"name\"]\n",
    "    if datasource == 'web_search':\n",
    "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
    "        return \"web_search\"\n",
    "    elif datasource == 'vectorstore':\n",
    "        print(\"---ROUTE QUESTION TO RAG---\")\n",
    "        return \"vectorstore\"\n",
    "    else: \n",
    "        print(\"---ROUTE QUESTION TO LLM---\")\n",
    "        return \"vectorstore\"\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, WEB SEARCH---\")\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n",
    "\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question,\"generation\": generation})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        pprint(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab01f36-5628-49ab-bfd3-84bb6f1a1b0f",
   "metadata": {
    "id": "3ab01f36-5628-49ab-bfd3-84bb6f1a1b0f"
   },
   "source": [
    "## Build Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67854e07-9293-4c3c-bf9a-bc9a605570ee",
   "metadata": {
    "id": "67854e07-9293-4c3c-bf9a-bc9a605570ee"
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"web_search\", web_search) # web search\n",
    "workflow.add_node(\"retrieve\", retrieve) # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents) # grade documents\n",
    "workflow.add_node(\"generate\", generate) # rag\n",
    "workflow.add_node(\"llm_fallback\", llm_fallback) # llm\n",
    "\n",
    "# Build graph\n",
    "workflow.set_conditional_entry_point(\n",
    "    route_question,\n",
    "    {\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"vectorstore\": \"retrieve\",\n",
    "        \"llm_fallback\": \"llm_fallback\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\", # Hallucinations: re-generate \n",
    "        \"not useful\": \"web_search\", # Fails to answer question: fall-back to web-search \n",
    "        \"useful\": END,\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"llm_fallback\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29acc541-d726-4b75-84d1-a215845fe88a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29acc541-d726-4b75-84d1-a215845fe88a",
    "outputId": "47caec8e-54e3-4f89-dfbb-94fc034666f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO WEB SEARCH---\n",
      "---WEB SEARCH---\n",
      "\"Node 'web_search':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "'The Bears are expected to draft Caleb Williams with their first pick.'\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": \"What player are the Bears expected to draft first in the 2024 NFL draft?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint.pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "    pprint.pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint.pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fddd00-58bf-4910-bf36-be9e5bfba778",
   "metadata": {
    "id": "11fddd00-58bf-4910-bf36-be9e5bfba778"
   },
   "source": [
    "Trace:\n",
    "\n",
    "https://smith.langchain.com/public/623da7bb-84a7-4e53-a63e-7ccd77fb9be5/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69a985dd-03c6-45af-a67b-b15746a2cb5f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69a985dd-03c6-45af-a67b-b15746a2cb5f",
    "outputId": "e5f799cc-6f36-494f-c8b2-192de1edb7fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO RAG---\n",
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "'Sensory, short-term, and long-term memory.'\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": \"What are the types of agent memory?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint.pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint.pprint(value [\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf41097-fc4c-4072-95b3-e7e07731ada1",
   "metadata": {
    "id": "ebf41097-fc4c-4072-95b3-e7e07731ada1"
   },
   "source": [
    "Trace:\n",
    "\n",
    "https://smith.langchain.com/public/57f3973b-6879-4fbe-ae31-9ae524c3a697/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "qPwP_2PNiOjQ",
   "metadata": {
    "id": "qPwP_2PNiOjQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---ROUTE QUESTION---\n",
      "---ROUTE QUESTION TO LLM---\n",
      "---LLM Fallback---\n",
      "\"Node 'llm_fallback':\"\n",
      "'\\n---\\n'\n",
      "(\"I don't have feelings as an AI assistant, but I'm here to help you with your \"\n",
      " 'queries. How can I assist you today?')\n"
     ]
    }
   ],
   "source": [
    "# Run\n",
    "inputs = {\"question\": \"Hello, how are you today?\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint.pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint.pprint(value [\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4107c8a4-6171-4c1b-840a-77a3d09f84fc",
   "metadata": {},
   "source": [
    "Trace: \n",
    "\n",
    "https://smith.langchain.com/public/1f628ee4-8d2d-451e-aeb1-5d5e0ede2b4f/r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3cda0a-c4bd-41ea-830b-d992f27fde15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
